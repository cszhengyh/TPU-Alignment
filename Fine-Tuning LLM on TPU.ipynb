{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes -q\n",
    "!pip3 install accelerate datasets sentencepiece langchain torch_xla[tpuvm] -q\n",
    "!pip uninstall -y tensorflow # `tensorflow` can conflict with `torch-xla`\n",
    "!pip install tensorflow-cpu -q # not must\n",
    "!git clone https://github.com/IsNoobgrammer/Pytorch-Optimizers optims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "get_ipython().kernel.do_shutdown(True) # True means restarting after kernel stoping\n",
    "### for good measures restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <hf_read_token> #for downloading gated models\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharding Module for different Architechture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:43:54.450080Z",
     "iopub.status.busy": "2024-02-29T08:43:54.449856Z",
     "iopub.status.idle": "2024-02-29T08:43:54.461349Z",
     "shell.execute_reply": "2024-02-29T08:43:54.460731Z",
     "shell.execute_reply.started": "2024-02-29T08:43:54.450051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spmd_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spmd_util.py\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import torch_xla.experimental.xla_sharding as xs\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import (\n",
    "    GPTNeoXConfig, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config, MixtralConfig, PhiConfig,GemmaConfig\n",
    ")\n",
    "\n",
    "# ends with $ to prevent sharding lora parameters\n",
    "\n",
    "\n",
    "T5_RULES = (\n",
    "    # embeddings\n",
    "    (\"shared$\", (\"mp\", \"fsdp\")),\n",
    "    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n",
    "    \n",
    "    # attention\n",
    "    (\"q$\", (\"fsdp\", \"mp\")),\n",
    "    (\"k$\", (\"fsdp\", \"mp\")),\n",
    "    (\"v$\", (\"fsdp\", \"mp\")),\n",
    "    (\"o$\", (\"mp\", \"fsdp\")),\n",
    "\n",
    "    # mlp\n",
    "    (\"w$\", (\"fsdp\", \"mp\")),\n",
    "    (\"wi_0$\", (\"fsdp\", \"mp\")),\n",
    "    (\"wi_1$\", (\"fsdp\", \"mp\")),\n",
    "    (\"wo$\", (\"mp\", \"fsdp\")),\n",
    "\n",
    "    # seq2seq lm head\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    ")\n",
    "\n",
    "QWEN_RULES = (\n",
    "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
    "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    "    )\n",
    "GPT2_RULES = (\n",
    "    # embeddings\n",
    "    (\"wte\", (\"mp\", \"fsdp\")), \n",
    "    (\"wpe\", (\"mp\", \"fsdp\")),\n",
    "    \n",
    "    # attention\n",
    "    (\"c_attn\", (\"fsdp\", \"mp\")),\n",
    "    (\"c_proj\", (\"mp\", \"fsdp\")),\n",
    "    \n",
    "    # mlp\n",
    "    (\"c_fc\", (\"fsdp\", \"mp\")), \n",
    "    (\"c_proj\", (\"mp\", \"fsdp\")),\n",
    "    \n",
    "    # output \n",
    "    (\"ln_f\", (\"fsdp\", \"mp\")),\n",
    ")\n",
    "MISTRAL_RULES = (\n",
    "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
    "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.(gate_proj|up_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    "    )\n",
    "\n",
    "\n",
    "PHI_RULES = (\n",
    "    ### (regex) linear modules, (list[sharding methods]) )\n",
    "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
    "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"self_attn\\\\.dense\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.fc2\", (\"mp\", \"fsdp\")),  \n",
    "    (\"mlp\\\\.fc1\", (\"fsdp\", \"mp\")),\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    "    \n",
    ")\n",
    "\n",
    "LLAMA_RULES = (\n",
    "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
    "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    "    )\n",
    "\n",
    "GPTNEOX_RULES = (\n",
    "    # embeddings\n",
    "    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n",
    "    # atention\n",
    "    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n",
    "    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n",
    "    # mlp\n",
    "    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n",
    "    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n",
    "    # output\n",
    "    (\"embed_out\", (\"fsdp\", \"mp\")),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "MIXTRAL_RULES = (\n",
    "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
    "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"w1\", (\"fsdp\", \"mp\")),\n",
    "    (\"w2\", (\"mp\", \"fsdp\")),\n",
    "    (\"w3\", (\"fsdp\", \"mp\")),\n",
    "    (\"gate\", (\"mp\", \"fsdp\")),\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    "    )\n",
    "\n",
    "GEMMA_RULES = (\n",
    "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
    "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    ")\n",
    "\n",
    "# xxConfig is the return value of model.config.__class__\n",
    "ALL_RULES = [\n",
    "    (GPTNeoXConfig, GPTNEOX_RULES),\n",
    "    (T5Config, T5_RULES),\n",
    "    (LlamaConfig, LLAMA_RULES),\n",
    "    (GPT2Config, GPT2_RULES),\n",
    "    (MistralConfig, MISTRAL_RULES),\n",
    "    (Qwen2Config, QWEN_RULES),\n",
    "    (MixtralConfig, MIXTRAL_RULES),\n",
    "    (PhiConfig,PHI_RULES),\n",
    "    (GemmaConfig,GEMMA_RULES),\n",
    "]\n",
    "\n",
    "\n",
    "def find_rule(model):\n",
    "    for config, rule in ALL_RULES:\n",
    "        x1=(str(config).split(\".\"))[-1]\n",
    "        x2=(str(model.config.__class__).split(\".\"))[-1]\n",
    "#         print(x1,x2)\n",
    "        if x1.lower()==x2.lower():\n",
    "            return rule\n",
    "    raise Exception(\"unsupported model to partitioning\")\n",
    "\n",
    "strkey2id = {\n",
    "    \"dp\": 0, ## usefull for sharding inputs\n",
    "    \"fsdp\": 1, ## Pytorch-Xla (2D-sharding) axis to shard data (mostly mesh shape will be (8,1)) data will be sharded 8 way \n",
    "    \"mp\": 2 ## axis to shard model model will be sharded one way \n",
    "               ## Recommened checking Pytorch-tpu/transfomers on github (xla-fork of transformers)\n",
    "}\n",
    "\n",
    "# xm.xla_device(): cant get free TPU automatically, 0 is default. https://chatgpt.com/share/673cc416-60b8-8001-bdab-702e2f9f252c\n",
    "def partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n",
    "    partition_specs = find_rule(model)\n",
    "    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n",
    "        \n",
    "    # print(rule)\n",
    "\n",
    "    # model.named_modules(): https://chatgpt.com/share/673cccdf-5f48-8001-904b-b53cea076d98\n",
    "    for name, module in model.named_modules():\n",
    "        module.to(device)\n",
    "#         print(name, module.__class__.__name__)\n",
    "        if isinstance(module, (nn.Embedding, nn.Linear)):\n",
    "            for rule_pattern, spec in rule:\n",
    "                if re.findall(rule_pattern, name.lower())  : # and (\"lora\" not in name.lower()):\n",
    "                    if verbose:\n",
    "                        print(\"match\", rule_pattern, name)\n",
    "                    \n",
    "                    xs.mark_sharding(module.weight, mesh, spec)\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:04.153806Z",
     "iopub.status.busy": "2024-02-29T08:44:04.153497Z",
     "iopub.status.idle": "2024-02-29T08:44:09.108646Z",
     "shell.execute_reply": "2024-02-29T08:44:09.107014Z",
     "shell.execute_reply.started": "2024-02-29T08:44:04.153780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import torch_xla.experimental.xla_sharding as xs\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import (\n",
    " AutoTokenizer, AutoModelForCausalLM, set_seed, DataCollatorWithPadding, AutoConfig \n",
    ")\n",
    "\n",
    "from transformers import logging as hf_logging\n",
    "import torch_xla.runtime as xr\n",
    "\n",
    "xr.use_spmd()\n",
    "\n",
    "from torch_xla.experimental.xla_sharding import Mesh\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model \n",
    "from datasets import  load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch_xla.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n",
    "    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
    "    os.environ.pop('TPU_PROCESS_ADDRESSES')\n",
    "    os.environ.pop('CLOUD_TPU_TASK_ID')\n",
    "    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:09.111498Z",
     "iopub.status.busy": "2024-02-29T08:44:09.110728Z",
     "iopub.status.idle": "2024-02-29T08:44:09.116089Z",
     "shell.execute_reply": "2024-02-29T08:44:09.115239Z",
     "shell.execute_reply.started": "2024-02-29T08:44:09.111464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_INPUT=4096 #128*32\n",
    "MODEL = \"fhai50032/RolePlayLake-7B\" #You should be able to use 7B model with no changes! There should be enough HBM\n",
    "SAVED_MODEL = \"fhai50032/RP-check-TPU\"\n",
    "# !export XLA_TENSOR_ALLOCATOR_MAXSIZE=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:16.939058Z",
     "iopub.status.busy": "2024-02-29T08:44:16.938665Z",
     "iopub.status.idle": "2024-02-29T08:44:17.868368Z",
     "shell.execute_reply": "2024-02-29T08:44:17.867377Z",
     "shell.execute_reply.started": "2024-02-29T08:44:16.939026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens :\n",
      "\n",
      " {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']} \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "if 'pad_token' not in tokenizer.special_tokens_map:\n",
    "  tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:21.271079Z",
     "iopub.status.busy": "2024-02-29T08:44:21.270023Z",
     "iopub.status.idle": "2024-02-29T08:44:21.279047Z",
     "shell.execute_reply": "2024-02-29T08:44:21.278123Z",
     "shell.execute_reply.started": "2024-02-29T08:44:21.271039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(TorchDataset):\n",
    "    def __init__(self, tokenizer, max_length=1024, dataset=None):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        messages = self.dataset[idx][\"conversations\"]\n",
    "        text = \"\"\n",
    "        for message in messages:\n",
    "            role = message[\"from\"]\n",
    "            if role == \"system\":\n",
    "                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n",
    "            if role in [\"human\",\"user\"]:\n",
    "                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n",
    "            if role == \"function-call\":\n",
    "                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n",
    "            if role == \"function-response\":\n",
    "                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n",
    "            if role in [\"gpt\",\"assistant\"]:\n",
    "                text += f\"<|im_start|>assistant\\n{message['value']}\"\n",
    "        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n",
    "            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\":input_ids[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:23.988961Z",
     "iopub.status.busy": "2024-02-29T08:44:23.988526Z",
     "iopub.status.idle": "2024-02-29T08:44:25.872432Z",
     "shell.execute_reply": "2024-02-29T08:44:25.871750Z",
     "shell.execute_reply.started": "2024-02-29T08:44:23.988927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset=\"fhai50032/magicoder-oss-instruct-sharegpt-75k\"\n",
    "test_dataset=\"fhai50032/magicoder-oss-instruct-sharegpt-75k\"\n",
    "\n",
    "train_data = load_dataset(train_dataset, split=\"train\").shuffle(seed=69)\n",
    "val = (load_dataset(test_dataset, split=\"train[:640]\")).shuffle(seed=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:27.104591Z",
     "iopub.status.busy": "2024-02-29T08:44:27.104163Z",
     "iopub.status.idle": "2024-02-29T08:44:27.112426Z",
     "shell.execute_reply": "2024-02-29T08:44:27.111804Z",
     "shell.execute_reply.started": "2024-02-29T08:44:27.104539Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6866"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:30.130826Z",
     "iopub.status.busy": "2024-02-29T08:44:30.130506Z",
     "iopub.status.idle": "2024-02-29T08:44:30.136084Z",
     "shell.execute_reply": "2024-02-29T08:44:30.135490Z",
     "shell.execute_reply.started": "2024-02-29T08:44:30.130799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FLAGS = {'MAX_INPUT': MAX_INPUT,\n",
    "         'LOGGING_STEPS': 1,\n",
    "         'NUM_EPOCHS': 1,\n",
    "         'PAUSE_STEPS':1000, # asks to exit training after x steps , #todo checkpoint saving me no lazy\n",
    "         'MAX_STEPS': -1,#Ooverides num epochs\n",
    "         'BATCH_SIZE': 2, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n",
    "          'LEN_TRAIN_DATA': len(train_data),\n",
    "         'VAL_STEPS': 20,\n",
    "         'VAL_BATCH': 5,\n",
    "         'GRAD_ACCUMULATION_STEP':1,\n",
    "         'MAX_GRAD_CLIP':1,\n",
    "        'LEARNING_RATE':6e-5,\n",
    "         'WARMUP_RATIO':0.01,\n",
    "         'OPTIMIZER':'SM3', # default = 'adamw'  options->  ['adamw','SM3','came','adafactor','lion']           \n",
    "         'SCHEDULAR':'cosine', # default= 'cosine'     options:-> ['linear','cosine']\n",
    "         'WEIGHT_DECAY':0.1,\n",
    "         'TRAIN_DATASET':train_dataset,\n",
    "         \"TEST_DATASET\":test_dataset,\n",
    "         'WANDB':True,\n",
    "        'PROJECT':'RP-Coder',\n",
    "        } # Indian pun :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantization When??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:37.569081Z",
     "iopub.status.busy": "2024-02-29T08:44:37.568732Z",
     "iopub.status.idle": "2024-02-29T08:44:42.171091Z",
     "shell.execute_reply": "2024-02-29T08:44:42.170417Z",
     "shell.execute_reply.started": "2024-02-29T08:44:37.569051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16) \n",
    "# model._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint) \n",
    "# gradient checkpointing is not properly setup needs to do barieer optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA Applicable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:44.631788Z",
     "iopub.status.busy": "2024-02-29T08:44:44.631468Z",
     "iopub.status.idle": "2024-02-29T08:44:49.349528Z",
     "shell.execute_reply": "2024-02-29T08:44:49.348777Z",
     "shell.execute_reply.started": "2024-02-29T08:44:44.631761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "\n",
      "trainable params: 150,011,904 || all params: 8,687,692,800 || trainable%: 1.7267174087923551\n"
     ]
    }
   ],
   "source": [
    "ls=LoraConfig(\n",
    "    r = 48, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n",
    "    target_modules = ['q_proj', 'down_proj', 'up_proj', 'o_proj', 'v_proj', 'gate_proj', 'k_proj'],\n",
    "    lora_alpha = 16, #weight_scaling\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimize\n",
    "    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n",
    ")\n",
    "model = get_peft_model(model, ls)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data-Distributer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:52.906598Z",
     "iopub.status.busy": "2024-02-29T08:44:52.906017Z",
     "iopub.status.idle": "2024-02-29T08:44:52.913549Z",
     "shell.execute_reply": "2024-02-29T08:44:52.912939Z",
     "shell.execute_reply.started": "2024-02-29T08:44:52.906562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Steps:- 430  , Each Step has 16 inputs\n"
     ]
    }
   ],
   "source": [
    "train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=FLAGS['MAX_INPUT'])\n",
    "val = ConversationDataset(tokenizer, dataset=val)\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\n",
    "training_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\n",
    "testing_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n",
    "\n",
    "print(f\"Max Steps:- {len(training_loader)}  , eFFECTIVE bATCH size {8*FLAGS['BATCH_SIZE']} Input\")\n",
    "print(f\"Val Size:- {len(testing_loader)}  , eFFECTIvE bATCH size {8*FLAGS['BATCH_SIZE']} Input\")\n",
    "FLAGS['STEPS']=len(training_loader)\n",
    "FLAGS['BATCH_DATA']=FLAGS['BATCH_SIZE']*8 ## 8 CORES ON TPU \n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(val[0]['input_ids'])\n",
    "c=0\n",
    "for step, batch in enumerate(training_loader):\n",
    "\n",
    "    print(step)\n",
    "    print(tokenizer.decode(batch['input_ids'][0]))\n",
    "    break\n",
    "# print(tokenizer.decode(val[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:44:57.462890Z",
     "iopub.status.busy": "2024-02-29T08:44:57.462566Z",
     "iopub.status.idle": "2024-02-29T08:44:57.468968Z",
     "shell.execute_reply": "2024-02-29T08:44:57.468232Z",
     "shell.execute_reply.started": "2024-02-29T08:44:57.462863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_nb_trainable_parameters(model):\n",
    "        r\"\"\"\n",
    "        Returns the number of trainable parameters and number of all parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            num_params = param.numel()\n",
    "            # if using DS Zero 3 and the weights are initialized empty\n",
    "            if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "                num_params = param.ds_numel\n",
    "\n",
    "            # Due to the design of 4bit linear layers from bitsandbytes\n",
    "            # one needs to multiply the number of parameters by 2 to get\n",
    "            # the correct number of parameters\n",
    "            if param.__class__.__name__ == \"Params4bit\":\n",
    "                num_params = num_params * 2\n",
    "\n",
    "            all_param += num_params\n",
    "            if param.requires_grad:\n",
    "                trainable_params += num_params\n",
    "\n",
    "        return trainable_params, all_param\n",
    "def print_trainable_parameters(model):\n",
    "        \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params, all_param = get_nb_trainable_parameters(model)\n",
    "        \n",
    "        print(\n",
    "            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:45:00.968977Z",
     "iopub.status.busy": "2024-02-29T08:45:00.968642Z",
     "iopub.status.idle": "2024-02-29T08:45:26.316300Z",
     "shell.execute_reply": "2024-02-29T08:45:26.315190Z",
     "shell.execute_reply.started": "2024-02-29T08:45:00.968948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from spmd_util import partition_module\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "device = xm.xla_device()\n",
    "model = model.to(device)\n",
    "from torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear  \n",
    "model = apply_xla_patch_to_nn_linear(model, xs.xla_patched_nn_linear_forward)  #for patching linear layer to use einsum instead of matmul\n",
    "num_devices = xr.global_runtime_device_count()\n",
    "model_axis=1\n",
    "data_axis=num_devices//model_axis\n",
    "mesh_shape = (1,data_axis, model_axis )\n",
    "device_ids = np.array(range(num_devices))\n",
    "mesh = Mesh(device_ids, mesh_shape, ('dp','fsdp', 'mp'))\n",
    "partition_module(model, mesh)\n",
    "training_loader = pl.MpDeviceLoader(training_loader, device)\n",
    "testing_loader = pl.MpDeviceLoader(testing_loader, device)\n",
    "mesh_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T08:45:34.901746Z",
     "iopub.status.busy": "2024-02-29T08:45:34.901138Z",
     "iopub.status.idle": "2024-02-29T08:45:36.417079Z",
     "shell.execute_reply": "2024-02-29T08:45:36.415928Z",
     "shell.execute_reply.started": "2024-02-29T08:45:34.901690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!export XLA_USE_BF16=1\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "__wandb__=FLAGS['WANDB']\n",
    "from torch_xla.amp.syncfree import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n",
    "from optims.optim import SM3, CAME , Adafactor\n",
    "# from random import randrange\n",
    "# from bitsandbytes.optim import AdamW8bit \n",
    "# from torchdistx.optimizers import AnyPrecisionAdamW\n",
    "\n",
    "val_step=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_loss(outputs,labels,pad_id=tokenizer.pad_token_id):\n",
    "  epsilon=1e-8\n",
    "  logits=outputs.logits\n",
    "  logits = logits[..., :-1, :].contiguous()\n",
    "  labels = labels[..., 1:].contiguous()\n",
    "  log_probs = -nn.functional.log_softmax(logits, dim=-1)\n",
    "  if labels.dim() == log_probs.dim() - 1:\n",
    "    labels = labels.unsqueeze(-1)\n",
    "  padding_mask = labels.eq(pad_id)\n",
    "  labels = torch.clamp(labels, min=0)\n",
    "  nll_loss = log_probs.gather(dim=-1, index=labels)\n",
    "  smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.bfloat16)\n",
    "  nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "  smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
    "  num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
    "  nll_loss = nll_loss.sum() / num_active_elements\n",
    "  smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
    "  del labels,logits,padding_mask\n",
    "  return (1-epsilon)*nll_loss + epsilon*smoothed_loss\n",
    "\n",
    "\n",
    "\n",
    "def train(FLAGS):\n",
    "\n",
    "    \n",
    "    ### Configuring Training\n",
    "    global val_step\n",
    "    update_params= filter(lambda p: p.requires_grad, model.parameters())\n",
    "    num_iterations = int((FLAGS[\"NUM_EPOCHS\"] * FLAGS['STEPS'] ) // FLAGS['GRAD_ACCUMULATION_STEP'])\n",
    "    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n",
    "    \n",
    "    if __wandb__:\n",
    "        wandb.init(project=FLAGS['PROJECT'],config=FLAGS)\n",
    "        wandb.define_metric(\"Validation_loss\", step_metric=\"val_step\")\n",
    "        wandb.define_metric(\"Learning_rate\",step_metric=\"train_step\")\n",
    "        wandb.define_metric(\"train_loss\",step_metric=\"train_step\")\n",
    "    \n",
    "    ### Optimizers\n",
    "    \n",
    "    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n",
    "        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n",
    "    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n",
    "        optimizer = Lion(update_params, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n",
    "    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n",
    "        optimizer = Adafactor(update_params,lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=False,relative_step=False)\n",
    "    elif (FLAGS['OPTIMIZER']).lower()=='came':\n",
    "        optimizer = CAME(model.parameters(),lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],betas=(0.9, 0.999, 0.9999),eps=(1e-30, 1e-16))\n",
    "    else:\n",
    "#         optimizer = Lilith(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n",
    "        optimizer=SM3(update_params,lr=FLAGS['LEARNING_RATE'])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if len(param_group[\"params\"]) > 0:\n",
    "            print(param_group[\"params\"][0].device)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    ### Schedulars\n",
    "    \n",
    "    if (FLAGS['SCHEDULAR']).lower()=='linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n",
    "    else:\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    ### Training Loop\n",
    "    val_step=0\n",
    "    check=False #for brakes\n",
    "    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n",
    "        if check:\n",
    "            break\n",
    "        model.train()\n",
    "        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n",
    "        for step, batch in enumerate(training_loader):\n",
    "            input_ids, labels,attention_mask = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device),batch['attention_mask'].to(device)\n",
    "            xs.mark_sharding(input_ids, mesh, (0,1))  ### earlier:-> (0,1) according to pytorch-xla , input/dataloaders must be sharded across ('data',None) \n",
    "            xs.mark_sharding( labels,  mesh, (0,1))  ###\n",
    "            xs.mark_sharding(  attention_mask,  mesh, (0, 1)) ###\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            loss = evaluate_loss(outputs,labels)\n",
    "            \n",
    "\n",
    "            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n",
    "                xm.master_print(f'loss: {loss.detach().cpu().item()}, time: {test_utils.now()}, step: {step+1}')\n",
    "            if __wandb__:\n",
    "                wandb.log({\n",
    "                'Learning_rate': optimizer.param_groups[0]['lr'],\n",
    "                'train_loss':  loss.detach().cpu().item(),\n",
    "                'train_step': step + 1 + ((epoch-1) * FLAGS[\"STEPS\"]),\n",
    "                        })\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            del input_ids , attention_mask \n",
    "            loss.backward()\n",
    "            del outputs,loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if (step+1) % FLAGS['GRAD_ACCUMULATION_STEP'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(update_params, max_norm=FLAGS['MAX_GRAD_CLIP']*8)\n",
    "                scheduler.step()\n",
    "                xm.optimizer_step(optimizer,pin_layout=True,barrier=True) ## performs xm.reduce_gradient() , optimizer.step() , xm.mark_step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if (step+1)% FLAGS['VAL_STEPS'] == 0:\n",
    "                end_index=FLAGS[\"VAL_BATCH\"]\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    total_loss = 0\n",
    "                    total_step = 0\n",
    "                    for stepx, batchx in enumerate(testing_loader):\n",
    "                        input_ids = batchx[\"input_ids\"].to(device)\n",
    "                        labels = batchx[\"labels\"].to(device)\n",
    "                        attention_mask = batchx[\"attention_mask\"].to(device)\n",
    "                        xs.mark_sharding(input_ids, mesh, (0, None))\n",
    "                        xs.mark_sharding(labels, mesh, (0, None))\n",
    "                        xs.mark_sharding( attention_mask,    mesh, (0, None))\n",
    "                        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "                        loss = evaluate_loss(outputs,labels)\n",
    "                        total_loss += loss.item()\n",
    "                        total_step +=1\n",
    "                        xm.master_print('----- Time -> {} ----- Validation Batch -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_step , loss.item()))\n",
    "                        if __wandb__:\n",
    "                            val_step+=1\n",
    "                            wandb.log({\n",
    "                                'Validation_loss': loss.item(),\n",
    "                                'val_step':val_step,\n",
    "                                    })\n",
    "                        if (stepx+1)%end_index==0:\n",
    "                            break\n",
    "                    model.train()    \n",
    "                    average_loss=total_loss/total_step\n",
    "                    xm.master_print('----- Time -> {} ----- Validation Batch Size -> {} ----  Validation Loss -> {:.7f}'.format(test_utils.now(), total_step , average_loss))\n",
    "\n",
    "            if (step+1)% FLAGS['PAUSE_STEPS']==0:\n",
    "                inp=input('want to continue training after {} steps'.format(step+1))\n",
    "                check = bool(\"no\" in inp.lower())\n",
    "                if check:\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12 Mins to Train on 4k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train(FLAGS)\n",
    "if FLAGS['WANDB']:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T09:09:58.342510Z",
     "iopub.status.busy": "2024-02-29T09:09:58.342138Z",
     "iopub.status.idle": "2024-02-29T09:12:07.731514Z",
     "shell.execute_reply": "2024-02-29T09:12:07.730373Z",
     "shell.execute_reply.started": "2024-02-29T09:09:58.342478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model on CPU\n",
      "\n",
      "Loaded model on cpu in 129.38400077819824 seconds \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('Loading the model on CPU')\n",
    "START=time.time()\n",
    "model = model.cpu()\n",
    "print(f\"Loaded model on cpu in {time.time()-START} seconds \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"hf_token\") ##\n",
    "model.push_to_hub(\n",
    "    SAVED_MODEL, \n",
    "    tokenizer=tokenizer,\n",
    "    safe_serialization=True,\n",
    "    create_pr=True,\n",
    "    max_shard_size=\"3GB\", \n",
    "    )\n",
    "tokenizer.push_to_hub(\n",
    "    SAVED_MODEL,\n",
    "    \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
